# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10z79DRBDlWiR_NjeioODAC98qyduNVWG
"""

!pip install evaluate nltk bert-score

!pip install rouge-score

# âœ… Evaluation Code
import pandas as pd
from evaluate import load
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import bert_score
import nltk
from rouge_score import rouge_scorer

# ðŸ“¥ Download NLTK tokenizer data
nltk.download("punkt")

from google.colab import drive
drive.mount('/content/drive')

# Load datasets
reference_df = pd.read_csv("/content/drive/MyDrive/DTSC 5082 Datasets/mimic-iv-bhc.csv")
generated_df = pd.read_csv("/content/drive/MyDrive/DTSC 5082 Datasets/summarized_1000_clinical_data.csv")

# Get summary columns (adjust column names if needed)
references = reference_df["target"].astype(str).tolist()
generated = generated_df["summary"].astype(str).tolist()

# Check the lengths of both lists
print(f"Length of predictions: {len(generated)}")
print(f"Length of references: {len(references)}")
# Ensure lengths match before computing ROUGE scores
if len(generated) == len(references):
    rouge_result = rouge.compute(predictions=generated, references=references, use_stemmer=True)
    print("\nROUGE Scores:")
    for k, v in rouge_result.items():
      print(f"{k}: {v:.4f}")
else:
    print("Mismatch in the number of predictions and references.")

# --- 2. BLEU ---
print("\n--- BLEU Score (average over all rows) ---")
smoothie = SmoothingFunction().method4
bleu_scores = [
    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)
    for ref, pred in zip(references, generated)
]
bleu_avg = sum(bleu_scores) / len(bleu_scores)
print(f"BLEU: {bleu_avg:.4f}")

# --- 3. BERTScore ---
# Ensure the same number of references and predictions
min_len = min(len(generated), len(references))
generated = generated[:min_len]
references = references[:min_len]

print("\n--- BERTScore ---")
P, R, F1 = bert_score.score(generated, references, lang="en", verbose=True)
print(f"Precision: {P.mean().item():.4f}")
print(f"Recall:    {R.mean().item():.4f}")
print(f"F1 Score:  {F1.mean().item():.4f}")